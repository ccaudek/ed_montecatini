{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#####  RUN NOTES\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "#####  PURPOSE\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "import datetime \n",
    "now = datetime.datetime.now()\n",
    "print (\"Current date and time : \")\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hddm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc\n",
    "import kabuki\n",
    "sns.set(style=\"white\")\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "# estimate convergence\n",
    "from kabuki.analyze import gelman_rubin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hddm.load_csv('rlddm_data.csv')\n",
    "#check structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model by calling hddm.HDDMrl (instead of hddm.HDDM for normal HDDM)\n",
    "m = hddm.HDDMrl(data)\n",
    "#set sample and burn-in\n",
    "m.sample(500,burn=100,dbname='traces.db',db='pickle')\n",
    "#print stats to get an overview of posterior distribution of estimated parameters\n",
    "m.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular RL without RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model by calling hddm.Hrl (instead of hddm.HDDM for normal model and hddm.HDDMrl for rlddm-model)\n",
    "m_rl = hddm.Hrl(data)\n",
    "#set sample and burn-in\n",
    "m_rl.sample(1500,burn=500,dbname='traces.db',db='pickle')\n",
    "#print stats to get an overview of posterior distribution of estimated parameters\n",
    "m_rl.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate convergence\n",
    "models = []\n",
    "for i in range(3):\n",
    "    m = hddm.Hrl(data=data)\n",
    "    m.sample(500, burn=200,dbname='traces.db',db='pickle')\n",
    "    models.append(m)\n",
    "#get max gelman-statistic value. shouldn't be higher than 1.1\n",
    "np.max(list(gelman_rubin(models).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelman_rubin(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the models we ran to test for convergence.\n",
    "m_rl = kabuki.utils.concat_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, v = m_rl.nodes_db.node[['alpha','v']]\n",
    "samples = {'alpha':alpha.trace(),'v':v.trace()}\n",
    "samp = pd.DataFrame(data=samples)\n",
    "\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .9), xycoords=ax.transAxes)\n",
    "\n",
    "g = sns.PairGrid(samp, palette=[\"red\"])\n",
    "g.map_upper(plt.scatter, s=10)\n",
    "g.map_diag(sns.distplot, kde=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "\n",
    "g.map_lower(corrfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe to store simulated data\n",
    "sim_data = pd.DataFrame()\n",
    "#create a column samp to be used to identify the simulated data sets\n",
    "data['samp'] = 0\n",
    "#load traces\n",
    "traces = m_rl.get_traces()\n",
    "#decide how many times to repeat simulation process. repeating this multiple times is generally recommended as it better captures the uncertainty in the posterior distribution, but will also take some time\n",
    "for i in tqdm(range(1,51)):\n",
    "    #randomly select a row in the traces to use for extracting parameter values\n",
    "    sample = np.random.randint(0,traces.shape[0]-1)\n",
    "    #loop through all subjects in observed data\n",
    "    for s in data.subj_idx.unique():\n",
    "        #get number of trials for each condition.\n",
    "        size0 = len(data[(data['subj_idx']==s) & (data['split_by']==0)].trial.unique())\n",
    "        size1 = len(data[(data['subj_idx']==s) & (data['split_by']==1)].trial.unique())\n",
    "        size2 = len(data[(data['subj_idx']==s) & (data['split_by']==2)].trial.unique())\n",
    "        #set parameter values for simulation\n",
    "        scaler = traces.loc[sample,'v_subj.'+str(s)]\n",
    "        alphaInv = traces.loc[sample,'alpha_subj.'+str(s)]\n",
    "        #take inverse logit of estimated alpha\n",
    "        alpha = np.exp(alphaInv)/(1+np.exp(alphaInv))\n",
    "        #simulate data for each condition changing only values of size, p_upper, p_lower and split_by between conditions.\n",
    "        sim_data0 = hddm.generate.gen_rand_rl_data(scaler=scaler,alpha=alpha,size=size0,p_upper=0.8,p_lower=0.2,split_by=0)\n",
    "        sim_data1 = hddm.generate.gen_rand_rl_data(scaler=scaler,alpha=alpha,size=size1,p_upper=0.7,p_lower=0.3,split_by=1)\n",
    "        sim_data2 = hddm.generate.gen_rand_rl_data(scaler=scaler,alpha=alpha,size=size2,p_upper=0.6,p_lower=0.4,split_by=2)\n",
    "        #append the conditions\n",
    "        sim_data0 = sim_data0.append([sim_data1,sim_data2],ignore_index=True)\n",
    "        #assign subj_idx\n",
    "        sim_data0['subj_idx'] = s\n",
    "        #identify that these are simulated data\n",
    "        sim_data0['type'] = 'simulated'\n",
    "        #identify the simulated data\n",
    "        sim_data0['samp'] = i\n",
    "        #append data from each subject\n",
    "        sim_data = sim_data.append(sim_data0,ignore_index=True)\n",
    "#combine observed and simulated data\n",
    "ppc_rl_data = data[['subj_idx','response','split_by','trial','feedback','samp']].copy()\n",
    "ppc_rl_data['type'] = 'observed'\n",
    "ppc_rl_sdata = sim_data[['subj_idx','response','split_by','trial','feedback','type','samp']].copy()\n",
    "ppc_rl_data = ppc_rl_data.append(ppc_rl_sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe to store simulated data\n",
    "sim_data = pd.DataFrame()\n",
    "#create a column samp to be used to identify the simulated data sets\n",
    "data['samp'] = 0\n",
    "#load traces\n",
    "traces = m_rl.get_traces()\n",
    "#decide how many times to repeat simulation process. repeating this multiple times is generally recommended as it better captures the uncertainty in the posterior distribution, but will also take some time\n",
    "for i in tqdm(range(1,11)):\n",
    "    #randomly select a row in the traces to use for extracting parameter values\n",
    "    sample = np.random.randint(0,traces.shape[0]-1)\n",
    "    #loop through all subjects in observed data\n",
    "    for s in data.subj_idx.unique():\n",
    "        #get number of trials for each condition.\n",
    "        size0 = len(data[(data['subj_idx']==s) & (data['split_by']==0)].trial.unique())\n",
    "        size1 = len(data[(data['subj_idx']==s) & (data['split_by']==1)].trial.unique())\n",
    "        size2 = len(data[(data['subj_idx']==s) & (data['split_by']==2)].trial.unique())\n",
    "        #set parameter values for simulation\n",
    "        scaler = traces.loc[sample,'v_subj.'+str(s)]\n",
    "        alphaInv = traces.loc[sample,'alpha_subj.'+str(s)]\n",
    "        #take inverse logit of estimated alpha\n",
    "        alpha = np.exp(alphaInv)/(1+np.exp(alphaInv))\n",
    "        #simulate data for each condition changing only values of size, p_upper, p_lower and split_by between conditions.\n",
    "        sim_data0 = hddm.generate.gen_rand_rl_data(scaler=scaler,alpha=alpha,size=size0,p_upper=0.8,p_lower=0.2,split_by=0)\n",
    "        sim_data1 = hddm.generate.gen_rand_rl_data(scaler=scaler,alpha=alpha,size=size1,p_upper=0.7,p_lower=0.3,split_by=1)\n",
    "        sim_data2 = hddm.generate.gen_rand_rl_data(scaler=scaler,alpha=alpha,size=size2,p_upper=0.6,p_lower=0.4,split_by=2)\n",
    "        #append the conditions\n",
    "        sim_data0 = sim_data0.append([sim_data1,sim_data2],ignore_index=True)\n",
    "        #assign subj_idx\n",
    "        sim_data0['subj_idx'] = s\n",
    "        #identify that these are simulated data\n",
    "        sim_data0['type'] = 'simulated'\n",
    "        #identify the simulated data\n",
    "        sim_data0['samp'] = i\n",
    "        #append data from each subject\n",
    "        sim_data = sim_data.append(sim_data0,ignore_index=True)\n",
    "#combine observed and simulated data\n",
    "ppc_rl_data = data[['subj_idx','response','split_by','trial','feedback','samp']].copy()\n",
    "ppc_rl_data['type'] = 'observed'\n",
    "ppc_rl_sdata = sim_data[['subj_idx','response','split_by','trial','feedback','type','samp']].copy()\n",
    "ppc_rl_data = ppc_rl_data.append(ppc_rl_sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for practical reasons we only look at the first 40 trials for each subject in a given condition\n",
    "plot_ppc_rl_data = ppc_rl_data[ppc_rl_data.trial<41].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin trials to for smoother estimate of response proportion across learning\n",
    "plot_ppc_rl_data['bin_trial'] = pd.cut(plot_ppc_rl_data.trial,11,labels=np.linspace(0, 10,11)).astype('int64')\n",
    "#calculate means for each sample\n",
    "sums = plot_ppc_rl_data.groupby(['bin_trial','split_by','samp','type']).mean().reset_index()\n",
    "#calculate the overall mean response across samples\n",
    "ppc_rl_sim = sums.groupby(['bin_trial','split_by','type']).mean().reset_index()\n",
    "#initiate columns that will have the upper and lower bound of the hpd\n",
    "ppc_rl_sim['upper_hpd'] = 0\n",
    "ppc_rl_sim['lower_hpd'] = 0\n",
    "for i in range(0,ppc_rl_sim.shape[0]):\n",
    "    #calculate the hpd/hdi of the predicted mean responses across bin_trials\n",
    "    hdi = pymc.utils.hpd(sums.response[(sums['bin_trial']==ppc_rl_sim.bin_trial[i]) & (sums['split_by']==ppc_rl_sim.split_by[i]) & (sums['type']==ppc_rl_sim.type[i])],alpha=0.1)\n",
    "    ppc_rl_sim.loc[i,'upper_hpd'] = hdi[1]\n",
    "    ppc_rl_sim.loc[i,'lower_hpd'] = hdi[0]\n",
    "#calculate error term as the distance from upper bound to mean\n",
    "ppc_rl_sim['up_err'] = ppc_rl_sim['upper_hpd']-ppc_rl_sim['response']\n",
    "ppc_rl_sim['low_err'] = ppc_rl_sim['response']-ppc_rl_sim['lower_hpd']\n",
    "ppc_rl_sim['model'] = 'RL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting evolution of choice proportion for best option across learning for observed and simulated data. Compared for RL and RLDDM models, both with single learnign rate.\n",
    "fig, axs = plt.subplots(figsize=(15, 5),nrows=1, ncols=3, sharex=True,sharey=True)\n",
    "for i in range(0,3):\n",
    "    ax = axs[i]\n",
    "    d_single = ppc_rl_sim[(ppc_rl_sim.split_by==i) & (ppc_rl_sim.type=='simulated')]\n",
    "    #slightly move bin_trial to avoid overlap in errorbars\n",
    "    d_single['bin_trial'] += 0.2\n",
    "    ax.errorbar(d_single.bin_trial, d_single.response, yerr=[d_single.low_err,d_single.up_err], label='simulated_RLDDM',color='orange')\n",
    "    ax = axs[i]\n",
    "    d_rl = ppc_rl_sim[(ppc_rl_sim.split_by==i) & (ppc_rl_sim.type=='simulated')]\n",
    "    ax.errorbar(d_rl.bin_trial, d_rl.response, yerr=[d_rl.low_err,d_rl.up_err], label='simulated_RL',color='green')\n",
    "    ax = axs[i]\n",
    "    # d = ppc_rl_sim[(ppc_dual_sim.split_by==i) & (ppc_dual_sim.type=='observed')]\n",
    "    # ax.plot(d.bin_trial, d.response,linewidth=3,label='observed')\n",
    "    ax.set_title('split_by = %i' %i,fontsize=20)\n",
    "    ax.set_ylabel('mean response')\n",
    "    ax.set_xlabel('trial')\n",
    "plt.xlim(-0.5,10.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of samples and burnins\n",
    "nsamples = 100\n",
    "nburn = 50\n",
    "\n",
    "m = hddm.HDDMnnRL(\n",
    "    data, \n",
    "    model='angle', \n",
    "    rl_rule='RWupdate', \n",
    "    non_centered=True, \n",
    "    include=['z', 'theta', 'rl_alpha'], \n",
    "    p_outlier = 0.0\n",
    ")\n",
    "m.sample(nsamples, burn=nburn, dbname='traces.db', db='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ssm = 'angle'\n",
    "model_rl = 'RWupdate'\n",
    "\n",
    "config_ssm = hddm.model_config.model_config[model_ssm]\n",
    "config_rl = hddm.model_config_rl.model_config_rl[model_rl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('rlssm_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_posteriors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hddm\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trace\n",
    "with open('./traces.db', 'rb') as handle:\n",
    "    tracefile = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-format traces as a dataframe\n",
    "traces = hddm.utils.get_traces_rlssm(tracefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ssm = 'angle'\n",
    "model_rl = 'RWupdate'\n",
    "\n",
    "config_ssm = hddm.model_config.model_config[model_ssm]\n",
    "config_rl = hddm.model_config_rl.model_config_rl[model_rl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hddm.plotting.plot_posterior_pairs_rlssm(tracefile, config_ssm['params'] + config_rl['params'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posterior_samples = 3\n",
    "p_lower = {0: 0.15, 1:0.30, 2:0.45}\n",
    "p_upper = {0: 0.85, 1:0.70, 2:0.55}\n",
    "ppc_sdata = hddm.plotting.gen_ppc_rlssm(model_ssm, config_ssm, model_rl, config_rl, data, traces, num_posterior_samples, p_lower, p_upper, save_data=True, save_name='ppc_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hddm.plotting.plot_ppc_choice_rlssm(data, ppc_sdata, 40, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hddm.plotting.plot_ppc_rt_rlssm(data, ppc_sdata, 40, 0.06)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pymc_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2262a597cd869e28977741846a3cdc375cf5b6b4c1a556a6c4e5292bea794205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
