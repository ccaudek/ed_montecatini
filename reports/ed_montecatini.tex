% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\usepackage{csquotes}
\usepackage{apacite}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{setspace}\doublespacing
\usepackage{float}
\floatplacement{figure}{H}
\floatplacement{table}{H}
\usepackage[font=small,skip=12pt]{caption}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Anorexia nervosa entails domain specific impairment of adaptive learning under uncertainty},
  pdfauthor={Corrado Caudek1, Claudio Sica3, Francesco Ceccarini1, Ilaria Colpizzi2, \& Virginia Alfei2},
  pdflang={en-EN},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Anorexia nervosa entails domain specific impairment of adaptive learning under uncertainty}
\author{Corrado Caudek\textsuperscript{1}, Claudio Sica\textsuperscript{3}, Francesco Ceccarini\textsuperscript{1}, Ilaria Colpizzi\textsuperscript{2}, \& Virginia Alfei\textsuperscript{2}}
\date{}


\shorttitle{Domain specific impairment of learning}

\authornote{

Correspondence concerning this article should be addressed to Corrado Caudek. E-mail: \href{mailto:corrado.caudek@unifi.it}{\nolinkurl{corrado.caudek@unifi.it}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Department of General Psychology, University of Padova, Italy\\\textsuperscript{2} NEUROFARBA Department, Psychology Section, University of Florence, Italy\\\textsuperscript{3} Health Sciences Department, Psychology Section, University of Florence, Italy}

\abstract{%
The abstract.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\url{https://doi.org/10.1007/s40167-018-0068-0}

To explore the processes underpinning task performance, computational modeling (i.e., drift diffusion model (DDM) analysis) will be used to explicate the specific processes by means of which domain specificigy influences decision-making (e.g., Golubickis et al.~2017, 2018; Macrae et al.~2017). In any task context, there are two distinct ways in which decisional processing can be biased. These pertain to how a stimulus is processed and how a response is generated, with each source of bias reflecting a different underlying component of decisional processing (Voss et al.~2004, 2013; White and Poldrack 2014). Whereas variability in stimulus processing affects the quality of information gathering during decision-making (i.e., dynamic stimulus bias), adjustments in response preparation influence how much evidence is required before a specific judgment is made (i.e., prior or pre-decisional bias). The theoretical value of a DDM analysis resides in its ability to isolate these independent forms of bias, thereby elucidate the component processes that underpin decision-making (Ratcliff 1978; Ratcliff and Rouder 1998; Ratcliff et al.~2016; Voss et al.~2004, 2013; Wagenmakers 2009).

The DDM assumes that, during binary decision-making (e.g., owned-by-self vs.~owned-by-other), noisy information is continuously sampled until sufficient evidence is acquired to initiate a response (see Fig. 1 for a schematic representation of the model). The duration of the diffusion process is known as the decision time, and the process itself can be characterized by several important parameters. Drift rate (v) estimates the speed of information gathering (i.e., larger drift rate = faster information uptake), thus is interpreted as a measure of the quality of visual processing during decision-making (White and Poldrack 2014). Boundary separa- tion (a) estimates the distance between the two decision thresholds (i.e., self-owned vs.~other-owned), hence indicates how much evidence is required before a response is made (i.e., larger (smaller) values indicate more conservative (liberal) respond- ing). The starting point (z) defines the position between the decision thresholds at which evidence accumulation begins. If z is not centered between the thresholds (i.e., z = 0.5), this denotes an a priori bias in favor of the response that is closer to the starting point (White and Poldrack 2014). In other words, less evidence is required to reach the preferred (vs.~non-preferred) threshold. Finally, the duration of all non-decisional processes is given by the additional parameter t0, which is taken to indicate biases in stimulus encoding and response execution (Voss et al.~2010).

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

\hypertarget{material}{%
\subsection{Material}\label{material}}

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

Participants who completed the first administration were prevented to be part of the second sample.

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

Data cleaning, manipulation, and analyses were performed in R program version 4.1.0 (R Core Team, 2020). Descriptive analyses were facilitated by \texttt{tidyverse} package (Wickham et al., 2019). Bayesian modeling to assess the associations between the truth discernment and overall belief and the available covariates was performed in Rstudio using Stan and associated packages. Bayesian regression models were fitted using the \texttt{brms} package (Bürkner, 2018). Markov chain Monte Carlo diagnostics for the final model were performed with (a) the potential scale reduction statistic (\(\hat{R}\)), (b) the ratio of the effective sample size to the total sample size drawn from the posterior distribution, and (c) trace plots of Markov chain Monte Carlo generated through the \texttt{bayesplot} package. Residual check and posterior predictive checks were also performed using the \texttt{bayesplot} package. The highest density interval of the posterior distribution was estimated using the \texttt{bayestestR} package.

Belief in Fake News was assessed by two common measures (Pennycook \& Rand, 2021): truth discernment and overall belief. Truth discernment was computed as the degree of belief in True News minus the degree of belief in Fake News.Truth discernment is akin to the sensitivity measure (\(d'\)) of the Signal Detection Theory {[}SDT; Green et al. (1966){]}. In our sample, we found a correlation of 0.98 between truth discernment and \(d'\). The \(d'\) index was computed by using a multilevel Bayesian ordinal regression to fit a hierarchical unequal variance SDT model to the participants' ratings of the news headlines, with clustering (\emph{i.e.}, random effects) on the levels of participants and items. Overall belief was computed as the sum of the degrees of belief in True News and Fake News (without distinguishing between them). This second index is akin to SDT `bias'. Prior to analysis, all continuous regression inputs were standardized to have a mean of 0 and a standard deviation of 1. As the values for predictors and the response were standardized, a weakly informative normal(0, 1) priors was used for the predictor slopes and intercept. Similarly, a weakly informative Cauchy(0, 1) prior was used for the model standard deviation (sigma).

The question of whether individuals show a similar vulnerability to Fake News when dealing with issues that have a strong direct personal relevance (such as the news about Covid-19), or with issues that have only a weak and indirect personal relevance (such as political news that do not have an immediate impact on the every-day life of the participant) was framed within the statistical problem of finding the optimal subset of the predictors that can maximally predict an outcome measure and that can optimally generalize to new, out-of-sample data.

Statistical techniques as stepwise selection, which is based on \emph{p}-values criterion or information criteria such as the AIC or BIC, or machine learning methods such as LASSO, in which the predictor slopes are continuously increased towards their least-squares solution, suffer from the problem of over-fitting. Because of over-fitting, models obtained with the previously mentioned techniques tend to provide a very good explain the data at hand (e.g., high \(R^2\), low RMSE), but their performance does not generalize and perform poorly when faced with new data. For such models, moreover, \emph{p}-values and confidence intervals are invalid because they are computed by ignoring the selection procedure.

The problem of finding the best predictors subset has recently found a new solution within a Bayesian framework in terms of the Predictive projection feature selection method (Piironen et al., 2020). At the first step, a reference model is fitted using all available predictors. At the second step, smaller submodels are fitted to approximate the reference model's predictions, using projection. The covariates in the best model for each submodel size are identified by decreasing the Kullback--Leibler divergence from the reference model to the projected submodel using a forward stepwise addition. The procedure selects the submodel with the smallest number of predictors which has similar predictive power as the reference model, judged by the mean log predictive density and the root mean square error. To avoid overfitting, at a third step the selected submodel is compared to the reference model on cross-validated prediction accuracy by using Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).

For covariate selection, the predictive projection technique proposed by Piironen et al.~was applied to the political data set. Then, cross validation was performed on the COVID-19 data set by using the optimal subset of predictors selected in the first step of the analysis. We reasoned as follows: If the best subset of predictors selected concerning the veracity judgments of political news does not reliably decrease predictive accuracy of a regression model applied to the veracity judgments of COVID-19 news, then we can safely conclude that Fake News vulnerability is not domain-specific (at least for the two extreme domains presently considered) in the sense that times of crisis, such that of the COVID-19 pandemic, affect susceptibility to misinformation in the similar manner as when the news have negligible personal relevance.

\hypertarget{decision-process-ddm}{%
\subsection{Decision process (DDM)}\label{decision-process-ddm}}

RT and accuracy data were simultaneously fit to DDM using the HDDM Python toolbox which implements Bayesian estimation of parameters with literature-based priors (Wiecki et al., 2013), following a generative-node hierarchical tree structure. Critically, to quantify and test the impact of our experimental conditions on the decision process, we built linear models over each DDM parameter (v, a, t, z, and we also included the accumulation rate inter-trial variability (sv) to account for slow errors (Ratcliff \& McKoon, 2008)). Because this was a hypothesis-driven approach, we defined a minimal model informed by theoretical constraints (Ratcliff \& McKoon, 2008) with only main effects for each fixed factor, except music condition. Since music condition could affect evidence threshold or accumulation rate non-exclusively, we compared models in which we conditioned either none, each or both parameters on music condition, by computing the Deviance Information Criterion (DIC) for each model. Then, we selected the model that had the lowest DIC (i.e., best trade-off between the quality of fit and model complexity) (winning DDM). Model validation and hypothesis testing followed the same rationale as for decision outcomes' GLMMs. We report the selected model specification with lme4-like syntax for clarity.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{preliminary-analysis-and-descriptive-statistics}{%
\subsection{Preliminary Analysis and Descriptive Statistics}\label{preliminary-analysis-and-descriptive-statistics}}

\hypertarget{classification}{%
\subsection{Classification}\label{classification}}

To estimate any potential advantage of the computational modeling approach, we trained two classifiers on disorder status. Both classifiers used individually-estimated DDM parameters but, in one case, the DDM parameters were obtained from the PRL task with neutral stimuli; in the other case, they were obtained from the PRL task with food stimuli. Importantly, these parameters were estimated from a model that did not have access to clinical status (i.e., all subjects were estimated with a single group distribution), to prevent classification bias that could otherwise arise due to shrinkage (an effect in hierarchical Bayesian model estimation where individual parameters can be estimated closer to the group mean). A logistic regression classifier was trained 100 times using 10-fold cross-validation. The best-performing classifier from the training was then used to iteratively predict diagnosis status on 30\% of held-out data. The performance of the classifier was measured on held-out data using the Area Under the Receiver-Operator-Curve (AUC) statistic, which can be interpreted to measure the probability of correctly choosing two randomly drawn samples from each the two classes (ED and controls).

\hypertarget{classification-of-clinical-status-based-on-computational-modeling}{%
\subsection{Classification of clinical status based on computational modeling}\label{classification-of-clinical-status-based-on-computational-modeling}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-R-brms_b}{}}%
Bürkner, P.-C. (2018). Advanced {Bayesian} multilevel modeling with the {R} package {brms}. \emph{The R Journal}, \emph{10}(1), 395--411. \url{https://doi.org/10.32614/RJ-2018-017}

\leavevmode\vadjust pre{\hypertarget{ref-green1966signal}{}}%
Green, D. M., Swets, J. A., et al. (1966). \emph{Signal detection theory and psychophysics} (Vol. 1). Wiley New York.

\leavevmode\vadjust pre{\hypertarget{ref-pennycook2021psychology}{}}%
Pennycook, G., \& Rand, D. G. (2021). The psychology of fake news. \emph{Trends in Cognitive Sciences}, \emph{25}(5), 388--402.

\leavevmode\vadjust pre{\hypertarget{ref-piironen2020projective}{}}%
Piironen, J., Paasiniemi, M., Vehtari, A., et al. (2020). Projective inference in high-dimensional problems: Prediction and feature selection. \emph{Electronic Journal of Statistics}, \emph{14}(1), 2155--2197.

\leavevmode\vadjust pre{\hypertarget{ref-R-base}{}}%
R Core Team. (2020). \emph{R: A language and environment for statistical computing}. R Foundation for Statistical Computing. \url{https://www.R-project.org/}

\leavevmode\vadjust pre{\hypertarget{ref-R-tidyverse}{}}%
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., \ldots{} Yutani, H. (2019). Welcome to the {tidyverse}. \emph{Journal of Open Source Software}, \emph{4}(43), 1686. \url{https://doi.org/10.21105/joss.01686}

\end{CSLReferences}


\clearpage
\makeatletter
\efloat@restorefloats
\makeatother


\begin{appendix}
\hypertarget{covid-19-news-data-set-full-model-with-13-covariates}{%
\section{COVID-19 news data set: full model with 13
covariates}\label{covid-19-news-data-set-full-model-with-13-covariates}}

\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table13covid}Posterior mean, standard error, 95\% credible interval and $\hat{R}$
    statistic for each parameter of the full model (13 covariates) predicting COVID-19 news truth discernment.}

\small{

\begin{tabular}{cccccc}
\toprule
parameter & \multicolumn{1}{c}{mean} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{lower bound} & \multicolumn{1}{c}{upper bound} & \multicolumn{1}{c}{Rhat}\\
\midrule
Intercept & -0.331 & 0.050 & -0.429 & -0.231 & 1.000\\
age & 0.166 & 0.036 & 0.094 & 0.236 & 1.000\\
agsu & 0.004 & 0.048 & -0.090 & 0.098 & 1.000\\
conv & -0.125 & 0.054 & -0.230 & -0.020 & 1.000\\
miis & -0.126 & 0.035 & -0.195 & -0.057 & 1.000\\
rfsp & -0.033 & 0.047 & -0.126 & 0.060 & 1.000\\
rfsn & -0.026 & 0.042 & -0.108 & 0.056 & 1.000\\
educ & 0.073 & 0.030 & 0.013 & 0.132 & 1.000\\
poli & 0.056 & 0.033 & -0.009 & 0.121 & 1.000\\
spir & 0.107 & 0.044 & 0.021 & 0.192 & 1.000\\
para & -0.069 & 0.039 & -0.145 & 0.007 & 1.000\\
crit & 0.157 & 0.029 & 0.100 & 0.215 & 1.000\\
cosp & -0.253 & 0.035 & -0.322 & -0.184 & 1.000\\
supe & -0.069 & 0.040 & -0.147 & 0.010 & 1.000\\
simp & -0.081 & 0.035 & -0.150 & -0.012 & 1.000\\
conf & 0.043 & 0.031 & -0.018 & 0.103 & 1.000\\
rese & -0.016 & 0.033 & -0.082 & 0.048 & 1.000\\
sex & 0.125 & 0.067 & -0.007 & 0.256 & 1.000\\
sigma & 0.749 & 0.020 & 0.711 & 0.790 & 1.000\\
\bottomrule
\end{tabular}

}

\end{threeparttable}
\end{center}

\end{table}

\newpage

\hypertarget{covid-19-news-data-set-best-projection-model-ignoring-the-political-news-data}{%
\section{COVID-19 news data set: best projection model (ignoring the
political news
data)}\label{covid-19-news-data-set-best-projection-model-ignoring-the-political-news-data}}

\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tablebestsubcovid}Posterior mean, standard error, 95\% credible interval and $\hat{R}$
    statistic for each parameter of the best projection model predicting COVID-19 news truth discernment.}

\small{

\begin{tabular}{cccccc}
\toprule
parameter & \multicolumn{1}{c}{mean} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{lower bound} & \multicolumn{1}{c}{upper bound} & \multicolumn{1}{c}{Rhat}\\
\midrule
Intercept & -0.368 & 0.043 & -0.453 & -0.283 & 1.000\\
miis & -0.142 & 0.034 & -0.209 & -0.075 & 1.000\\
cosp & -0.285 & 0.033 & -0.350 & -0.221 & 1.000\\
crit & 0.172 & 0.029 & 0.116 & 0.230 & 1.000\\
age & 0.188 & 0.032 & 0.126 & 0.250 & 1.000\\
supe & -0.122 & 0.036 & -0.193 & -0.052 & 1.000\\
conv & -0.142 & 0.048 & -0.235 & -0.049 & 1.000\\
sigma & 0.757 & 0.020 & 0.718 & 0.798 & 1.000\\
\bottomrule
\end{tabular}

}

\end{threeparttable}
\end{center}

\end{table}
\end{appendix}

\end{document}
